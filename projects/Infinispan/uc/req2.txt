[ISPN-374] Add event handling to Hot Rod [ISPN-3530] The HotRod client should support a separate CH for each cache [ISPN-833] Revisit cache name predefinition limitation for cache servers [ISPN-3529] Add support for the segment-based CH in the HotRod protocol [ISPN-4851] Make SyncConsistentHashFactory the default CH factory [ISPN-5413] Index Affinity support  <p>Enable Hot Rod servers to send events asynchronously to registered clients over persistent connections. This would allow use cases such as clients maintaining L1 caches and these events could be used for servers to invalidate keys on the client side. See dev list thread for more info.</p> <p>With asymmetric clusters, each cache can have its own consistent hash, so the primary owner of a key in one cache is not necessarily the owner in all the caches. Even with a symmetric cluster, the same client may be used to access both distributed and replicated caches, and those would certainly have a different CH.</p>

<p>In order to send the operations to the correct owner, the HotRod client should use a different CH for each cache.</p> <p>There re are two primary reasons why Infinispan servers require predefined caches to be started up on startup, and do not allow invocations to undefined caches:</p>

<p>1. Concurrent cache startups were resulting in NPEs (<a href= https://issues.jboss.org/browse/ISPN-635  title= NPE on acquire lock LockManagerImpl.lockAndRecord  class= issue-link  data-issue-key= ISPN-635 ><del>ISPN-635</del></a>) - This is already solved since the 4.2.x days. <br/>
2. Infinispan has issues dealing with asymmetric clusters (<a href= https://issues.jboss.org/browse/ISPN-658  title= Asymmetric clusters should be supported  class= issue-link  data-issue-key= ISPN-658 ><del>ISPN-658</del></a>).</p>

<p>Once these two issues have been resolved, revisit the limitation. </p> <p>The server CH changed in version 5.2 from virtual-nodes-based to segment-based, but the client CH stayed the same. The server is able to translate the server CH into a client CH, but the translation is imperfect:<br/>
1. Sometimes the client CH computes a different primary owner than the server CH.<br/>
2. The client CH gets slower as the number of segments increases, the server CH performance stays the same.</p> <p>With <a href= https://issues.jboss.org/browse/ISPN-4682  title= Improve SyncConsistentHashFactory key distribution  class= issue-link  data-issue-key= ISPN-4682 ><del>ISPN-4682</del></a> fixed, SyncConsistentHashFactory should be good enough to be the default. It still allows for more variation in the number of owned segments per node (+/-10% owned segments and +/-20% for primary-owned segments), but that should be acceptable for most purposes. </p>

<p>The major advantage of SCHF is that it depends only on the cache members and not on the order they joined. Users expect a key to map to the same node in all caches (as long as the caches have the same members).</p>

<p>One downside of SCHF, especially for testing, is that the segment ownership differs between test runs (being based on the random address assigned to each node). However, most tests that depend on key ownership should use <tt>ControlledConsistentHashFactory</tt> anyway.</p>

<p>We also need to verify that the number of segments moved by SCHF is comparable to the number of segments moved by DefaultConsistentHashFactory (<a href= https://issues.jboss.org/browse/ISPN-3729  title= Minimize the number of moved segments for SyncConsistentHashFactory  class= issue-link  data-issue-key= ISPN-3729 >ISPN-3729</a>).</p> <p>As described in <a href= https://github.com/infinispan/infinispan/wiki/Index-affinity-proposal  class= external-link  rel= nofollow >https://github.com/infinispan/infinispan/wiki/Index-affinity-proposal</a></p> 